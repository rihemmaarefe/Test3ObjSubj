{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rihemmaarefe/Test3ObjSubj/blob/main/test3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Davtrr9t5WYO",
        "outputId": "3e96ac5e-5914-4867-d9ba-8194dfc1f1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfWH6MZa5Zlu",
        "outputId": "93f2a6f7-eb50-4d37-9a39-26429f268a5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from altair) (1.5.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair) (3.1.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair) (0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair) (1.22.4)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (23.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.18->altair) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install altair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a2pW_0FsXGc",
        "outputId": "eddb8d30-db14-4439-9c72-99cf94c9b04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: TextBlob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from TextBlob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->TextBlob) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->TextBlob) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->TextBlob) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->TextBlob) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OAKutj45e6W",
        "outputId": "f61827f5-b4f4-46cf-f7ff-7018f162f886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following NEW packages will be installed:\n",
            "  sqlite3\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  base-files cuda-toolkit-config-common libasn1-8-heimdal libgnutls30\n",
            "  libgssapi3-heimdal libhcrypto4-heimdal libheimbase1-heimdal\n",
            "  libheimntlm0-heimdal libhx509-5-heimdal libkrb5-26-heimdal libpam-modules\n",
            "  libpam-modules-bin libpam-runtime libpam0g libroken18-heimdal libudev1\n",
            "  libwind0-heimdal linux-libc-dev openssl tar\n",
            "20 upgraded, 1 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 5,032 kB of archives.\n",
            "After this operation, 2,846 kB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  cuda-toolkit-config-common 12.1.105-1 [16.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 base-files amd64 11ubuntu5.7 [60.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 tar amd64 1.30+dfsg-7ubuntu0.20.04.3 [240 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam0g amd64 1.3.1-5ubuntu4.6 [55.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-modules-bin amd64 1.3.1-5ubuntu4.6 [41.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-modules amd64 1.3.1-5ubuntu4.6 [260 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-runtime all 1.3.1-5ubuntu4.6 [37.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev1 amd64 245.4-4ubuntu3.21 [75.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgnutls30 amd64 3.6.13-2ubuntu1.8 [829 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssl amd64 1.1.1f-1ubuntu2.18 [621 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libroken18-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [42.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libasn1-8-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [181 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libheimbase1-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [30.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libhcrypto4-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [88.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwind0-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [47.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libhx509-5-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [107 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libkrb5-26-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [207 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libheimntlm0-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [15.1 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgssapi3-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [96.5 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 linux-libc-dev amd64 5.4.0-148.165 [1,120 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 sqlite3 amd64 3.31.1-4ubuntu0.5 [860 kB]\n",
            "Fetched 5,032 kB in 0s (13.6 MB/s)\n",
            "Preconfiguring packages ...\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../base-files_11ubuntu5.7_amd64.deb ...\n",
            "Unpacking base-files (11ubuntu5.7) over (11ubuntu5.6) ...\n",
            "Setting up base-files (11ubuntu5.7) ...\n",
            "Installing new version of config file /etc/issue ...\n",
            "Installing new version of config file /etc/issue.net ...\n",
            "Installing new version of config file /etc/lsb-release ...\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../tar_1.30+dfsg-7ubuntu0.20.04.3_amd64.deb ...\n",
            "Unpacking tar (1.30+dfsg-7ubuntu0.20.04.3) over (1.30+dfsg-7ubuntu0.20.04.2) ...\n",
            "Setting up tar (1.30+dfsg-7ubuntu0.20.04.3) ...\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/sbin/rmt-tar because link group rmt is broken\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam0g_1.3.1-5ubuntu4.6_amd64.deb ...\n",
            "Unpacking libpam0g:amd64 (1.3.1-5ubuntu4.6) over (1.3.1-5ubuntu4.4) ...\n",
            "Setting up libpam0g:amd64 (1.3.1-5ubuntu4.6) ...\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-modules-bin_1.3.1-5ubuntu4.6_amd64.deb ...\n",
            "Unpacking libpam-modules-bin (1.3.1-5ubuntu4.6) over (1.3.1-5ubuntu4.4) ...\n",
            "Setting up libpam-modules-bin (1.3.1-5ubuntu4.6) ...\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-modules_1.3.1-5ubuntu4.6_amd64.deb ...\n",
            "Unpacking libpam-modules:amd64 (1.3.1-5ubuntu4.6) over (1.3.1-5ubuntu4.4) ...\n",
            "Setting up libpam-modules:amd64 (1.3.1-5ubuntu4.6) ...\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-runtime_1.3.1-5ubuntu4.6_all.deb ...\n",
            "Unpacking libpam-runtime (1.3.1-5ubuntu4.6) over (1.3.1-5ubuntu4.4) ...\n",
            "Setting up libpam-runtime (1.3.1-5ubuntu4.6) ...\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (245.4-4ubuntu3.21) over (245.4-4ubuntu3.19) ...\n",
            "Setting up libudev1:amd64 (245.4-4ubuntu3.21) ...\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libgnutls30_3.6.13-2ubuntu1.8_amd64.deb ...\n",
            "Unpacking libgnutls30:amd64 (3.6.13-2ubuntu1.8) over (3.6.13-2ubuntu1.7) ...\n",
            "Setting up libgnutls30:amd64 (3.6.13-2ubuntu1.8) ...\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../00-openssl_1.1.1f-1ubuntu2.18_amd64.deb ...\n",
            "Unpacking openssl (1.1.1f-1ubuntu2.18) over (1.1.1f-1ubuntu2.16) ...\n",
            "Preparing to unpack .../01-cuda-toolkit-config-common_12.1.105-1_all.deb ...\n",
            "Unpacking cuda-toolkit-config-common (12.1.105-1) over (12.0.146-1) ...\n",
            "Preparing to unpack .../02-libroken18-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libroken18-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../03-libasn1-8-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libasn1-8-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../04-libheimbase1-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libheimbase1-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../05-libhcrypto4-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libhcrypto4-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../06-libwind0-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libwind0-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../07-libhx509-5-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libhx509-5-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../08-libkrb5-26-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libkrb5-26-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../09-libheimntlm0-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libheimntlm0-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../10-libgssapi3-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libgssapi3-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../11-linux-libc-dev_5.4.0-148.165_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (5.4.0-148.165) over (5.4.0-137.154) ...\n",
            "Selecting previously unselected package sqlite3.\n",
            "Preparing to unpack .../12-sqlite3_3.31.1-4ubuntu0.5_amd64.deb ...\n",
            "Unpacking sqlite3 (3.31.1-4ubuntu0.5) ...\n",
            "Setting up cuda-toolkit-config-common (12.1.105-1) ...\n",
            "Setting up linux-libc-dev:amd64 (5.4.0-148.165) ...\n",
            "Setting up libroken18-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up openssl (1.1.1f-1ubuntu2.18) ...\n",
            "Setting up sqlite3 (3.31.1-4ubuntu0.5) ...\n",
            "Setting up libheimbase1-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libasn1-8-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libhcrypto4-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libwind0-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libhx509-5-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libkrb5-26-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libheimntlm0-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libgssapi3-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get upgrade sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zArh7K105j9"
      },
      "outputs": [],
      "source": [
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer \n",
        "# from sklearn.naive_bayes import MultinomialNB \n",
        "# from textblob import TextBlob\n",
        "# import base64\n",
        "\n",
        "# # Define a function to create a download link for a dataframe\n",
        "# def create_download_link(df, filename):\n",
        "#     csv = df.to_csv(index=False)\n",
        "#     b64 = base64.b64encode(csv.encode()).decode()\n",
        "#     href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">Download {filename}</a>'\n",
        "#     return href\n",
        "\n",
        "# # Load the data\n",
        "# uploaded_file = st.file_uploader(\"Choose a file\", type=\"csv\", accept_multiple_files=False)\n",
        "# if uploaded_file is not None:\n",
        "#     data = pd.read_csv(uploaded_file)\n",
        "\n",
        "#     # Train a machine learning model on a labeled dataset\n",
        "#     labeled_data = pd.read_csv(\"labeled_data.csv\")\n",
        "#     vectorizer = CountVectorizer(stop_words='english') \n",
        "#     X_counts = vectorizer.fit_transform(labeled_data['Text']) \n",
        "#     model = MultinomialNB() \n",
        "#     model.fit(X_counts, labeled_data['Category']) \n",
        "\n",
        "#     # Label the new data using the trained classifier\n",
        "#     X_new = vectorizer.transform(data['Text']) \n",
        "#     predicted_labels = model.predict(X_new)\n",
        "#     data['Predicted Category'] = predicted_labels\n",
        "\n",
        "#     # Save the labeled dataset to a new CSV file\n",
        "#     labeled_data_file = \"labeled_data.csv\"\n",
        "#     data.to_csv(labeled_data_file, index=False)\n",
        "\n",
        "#     st.success(\"Data labeled successfully and saved to labeled_data.csv\")\n",
        "\n",
        "#     # Display the labeled dataset\n",
        "#     st.write(\"Labeled dataset:\")\n",
        "#     st.write(data)\n",
        "\n",
        "#     # Add a download button for the labeled dataset\n",
        "#     st.markdown(create_download_link(data, labeled_data_file), unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO0m9AJI3jU1"
      },
      "outputs": [],
      "source": [
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# import base64\n",
        "\n",
        "# data = pd.read_csv('/content/drive/MyDrive/stage/trainData.csv') \n",
        "# X = data['meaning'] \n",
        "# y = data['category']\n",
        "\n",
        "# # Split the data into training and testing sets \n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Preprocess the data using the CountVectorizer \n",
        "# vectorizer = CountVectorizer(stop_words='english') \n",
        "# X_train_counts = vectorizer.fit_transform(X_train) \n",
        "# X_test_counts = vectorizer.transform(X_test) \n",
        "\n",
        "# # Train a logistic regression classifier\n",
        "# model = LogisticRegression()\n",
        "# model.fit(X_train_counts, y_train)\n",
        "\n",
        "# # Evaluate the model's accuracy on the testing set\n",
        "# y_pred = model.predict(X_test_counts)\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print('Logistic Regression Classifier Accuracy:', accuracy)\n",
        "\n",
        "# # Define a function to create a download link for a dataframe\n",
        "# def create_download_link(df, filename):\n",
        "#     csv = df.to_csv(index=False)\n",
        "#     b64 = base64.b64encode(csv.encode()).decode()\n",
        "#     href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">Download {filename}</a>'\n",
        "#     return href\n",
        "\n",
        "# # Load the data\n",
        "# uploaded_file = st.file_uploader(\"Choose a file\", type=\"csv\", accept_multiple_files=False)\n",
        "# if uploaded_file is not None:\n",
        "#     data = pd.read_csv(uploaded_file)\n",
        "\n",
        "#     # Preprocess the data using the CountVectorizer \n",
        "#     X_new_counts = vectorizer.transform(data['Text']) \n",
        "\n",
        "#     # Label the new data using the trained classifier\n",
        "#     predicted_labels = model.predict(X_new_counts)\n",
        "#     data['Predicted Category'] = predicted_labels\n",
        "\n",
        "#     # Save the labeled dataset to a new CSV file\n",
        "#     labeled_data_file = \"labeled_data.csv\"\n",
        "#     data.to_csv(labeled_data_file, index=False)\n",
        "\n",
        "#     st.success(\"Data labeled successfully and saved to labeled_data.csv\")\n",
        "\n",
        "#     # Display the labeled dataset\n",
        "#     st.write(\"Labeled dataset:\")\n",
        "#     st.write(data)\n",
        "\n",
        "#     # Add a download button for the labeled dataset\n",
        "#     st.markdown(create_download_link(data, labeled_data_file), unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6gtGiRs91ug"
      },
      "outputs": [],
      "source": [
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from textblob import TextBlob\n",
        "# import base64\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# data = pd.read_csv('/content/drive/MyDrive/stage/trainData.csv')\n",
        "# X = data['meaning']\n",
        "# y = data['category']\n",
        "\n",
        "# # Preprocess the data using the CountVectorizer\n",
        "# vectorizer = CountVectorizer(stop_words='english')\n",
        "# X_counts = vectorizer.fit_transform(X)\n",
        "\n",
        "# # Train a logistic regression classifier\n",
        "# model = LogisticRegression()\n",
        "# model.fit(X_counts, y)\n",
        "\n",
        "# # Define a function to create a download link for a dataframe\n",
        "# def create_download_link(df, filename):\n",
        "#     csv = df.to_csv(index=False)\n",
        "#     b64 = base64.b64encode(csv.encode()).decode()\n",
        "#     href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">Download {filename}</a>'\n",
        "#     return href\n",
        "\n",
        "# # Add a title and description to the app\n",
        "# st.title(\"Text Classification App\")\n",
        "# st.write(\"This app can classify text into objective and subjective categories.\")\n",
        "\n",
        "# # Add a text input field for user input\n",
        "# user_input = st.text_input(\"Enter some text:\")\n",
        "\n",
        "# # If user has entered text, predict its category and display the result\n",
        "# if user_input:\n",
        "#     user_input_counts = vectorizer.transform([user_input])\n",
        "#     predicted_label = model.predict(user_input_counts)[0]\n",
        "#     if predicted_label == 0:\n",
        "#         st.write(\"The text is classified as objective.\")\n",
        "#     else:\n",
        "#         st.write(\"The text is classified as subjective.\")\n",
        "        \n",
        "# # Add a file uploader for uploading a CSV file\n",
        "# uploaded_file = st.file_uploader(\"Upload a CSV file\", type=\"csv\")\n",
        "\n",
        "# # If a file is uploaded, label it using the trained classifier, save the labeled data to a new CSV file, and display it\n",
        "# if uploaded_file is not None:\n",
        "#     data = pd.read_csv(uploaded_file)\n",
        "#     X_new = vectorizer.transform(data['Text'])\n",
        "#     predicted_labels = model.predict(X_new)\n",
        "#     data['Predicted Category'] = predicted_labels\n",
        "#     labeled_data_file = \"labeled_data.csv\"\n",
        "#     data.to_csv(labeled_data_file, index=False)\n",
        "#     st.success(\"Data labeled successfully and saved to labeled_data.csv\")\n",
        "#     st.write(\"Labeled dataset:\")\n",
        "#     st.write(data)\n",
        "#     st.markdown(create_download_link(data, labeled_data_file), unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7gw_JRK2U13"
      },
      "outputs": [],
      "source": [
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.svm import LinearSVC\n",
        "# from textblob import TextBlob\n",
        "# import base64\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Load the data\n",
        "# data = pd.read_csv('/content/drive/MyDrive/stage/trainData.csv')\n",
        "\n",
        "# # Split the data into training and testing datasets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(data['meaning'], data['category'], test_size=0.2, random_state=42)\n",
        "\n",
        "# # Preprocess the data using the CountVectorizer\n",
        "# vectorizer = CountVectorizer(stop_words='english')\n",
        "# X_train_counts = vectorizer.fit_transform(X_train)\n",
        "# X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# # Train a logistic regression classifier\n",
        "# model_lr = LogisticRegression()\n",
        "# model_lr.fit(X_train_counts, y_train)\n",
        "\n",
        "# # Train a Naive Bayes classifier\n",
        "# model_nb = MultinomialNB()\n",
        "# model_nb.fit(X_train_counts, y_train)\n",
        "\n",
        "# # Train a Support Vector Machine classifier\n",
        "# model_svm = LinearSVC()\n",
        "# model_svm.fit(X_train_counts, y_train)\n",
        "\n",
        "# # Evaluate the accuracy of each model on the testing dataset\n",
        "# y_pred_lr = model_lr.predict(X_test_counts)\n",
        "# y_pred_nb = model_nb.predict(X_test_counts)\n",
        "# y_pred_svm = model_svm.predict(X_test_counts)\n",
        "\n",
        "# accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "# accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "# accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "# # Add a title and description to the app\n",
        "# st.title(\"Text Classification App\")\n",
        "# st.write(\"This app can classify text into objective and subjective categories.\")\n",
        "\n",
        "# # Add the accuracy of each model to the app\n",
        "# st.write(f\"Logistic Regression Classifier Accuracy: {accuracy_lr}\")\n",
        "# st.write(f\"Naive Bayes Classifier Accuracy: {accuracy_nb}\")\n",
        "# st.write(f\"Support Vector Machine Classifier Accuracy: {accuracy_svm}\")\n",
        "\n",
        "# # Add a text input field for user input\n",
        "# user_input = st.text_input(\"Enter some text:\")\n",
        "\n",
        "# # If user has entered text, predict its category using each classifier and display the results\n",
        "# if user_input:\n",
        "#     user_input_counts = vectorizer.transform([user_input])\n",
        "#     predicted_label_lr = model_lr.predict(user_input_counts)[0]\n",
        "#     predicted_label_nb = model_nb.predict(user_input_counts)[0]\n",
        "#     predicted_label_svm = model_svm.predict(user_input_counts)[0]\n",
        "#     st.write(\"Logistic Regression Classifier Prediction:\")\n",
        "#     if predicted_label_lr == \"subjective\":\n",
        "#         st.write(\"The text is classified as subjective.\")\n",
        "#     else:\n",
        "#         st.write(\"The text is classified as objective.\")\n",
        "#     st.write(\"Naive Bayes Classifier Prediction:\")\n",
        "#     if predicted_label_nb == \"subjective\":\n",
        "#         st.write(\"The text is classified as subjective.\")\n",
        "#     else:\n",
        "#         st.write(\"The text is classified as objective.\")\n",
        "#     st.write(\"Support Vector Machine Classifier Prediction:\")\n",
        "#     if predicted_label_svm == \"subjective\":\n",
        "#         st.write(\"The text is classified as subjective.\")\n",
        "#     else:\n",
        "#         st.write(\"The text is classified as objective.\")\n",
        "\n",
        "# # Add a file uploader for uploading a CSV file\n",
        "# uploaded_file = st.file_uploader(\"Upload a CSV file\", type=\"csv\")\n",
        "\n",
        "# # If a file is uploaded, label it using each classifier, save the labeled data to\n",
        "# if uploaded_file:\n",
        "#   # Load the data from the uploaded file\n",
        "#   data = pd.read_csv(uploaded_file)\n",
        "#   # Preprocess the data using the CountVectorizer\n",
        "# X_counts = vectorizer.transform(data['meaning'])\n",
        "\n",
        "# # Predict the category of each text using each classifier\n",
        "# predicted_labels_lr = model_lr.predict(X_counts)\n",
        "# predicted_labels_nb = model_nb.predict(X_counts)\n",
        "# predicted_labels_svm = model_svm.predict(X_counts)\n",
        "\n",
        "# # Add the predicted labels to the DataFrame\n",
        "# data['LR Prediction'] = predicted_labels_lr\n",
        "# data['NB Prediction'] = predicted_labels_nb\n",
        "# data['SVM Prediction'] = predicted_labels_svm\n",
        "\n",
        "# # Download the labeled file\n",
        "# csv = data.to_csv(index=False)\n",
        "# b64 = base64.b64encode(csv.encode()).decode()\n",
        "# href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"labeled_data.csv\">Download labeled data</a>'\n",
        "# st.markdown(href, unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "376Gs8vq5Bjf",
        "outputId": "904da9bf-dff8-43f4-a56b-5a86019c0857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from textblob import TextBlob\n",
        "import base64\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import re\n",
        "import string\n",
        "import sqlite3\n",
        "\n",
        "# Define the SessionState class to store the dataframe\n",
        "class SessionState:\n",
        "    def __init__(self):\n",
        "        self.results_df = pd.DataFrame(columns=['text_input', 'predicted_label'])\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    # Remove whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/drive/MyDrive/stage/trainData.csv')\n",
        "\n",
        "# Split the data into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['meaning'], data['category'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data using the CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a logistic regression classifier\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train_counts, y_train)\n",
        "\n",
        "# Train a BernoulliNB classifier\n",
        "model_bnb = BernoulliNB()\n",
        "model_bnb.fit(X_train_counts, y_train)\n",
        "\n",
        "# Train a Naive Bayes classifier\n",
        "model_nb = MultinomialNB()\n",
        "model_nb.fit(X_train_counts, y_train)\n",
        "\n",
        "# Train a Support Vector Machine classifier\n",
        "model_svm = LinearSVC()\n",
        "model_svm.fit(X_train_counts, y_train)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "model_rf = RandomForestClassifier()\n",
        "model_rf.fit(X_train_counts, y_train)\n",
        "\n",
        "# Train a Decision Tree classifier\n",
        "model_dt = DecisionTreeClassifier()\n",
        "model_dt.fit(X_train_counts, y_train)\n",
        "\n",
        "# Train K-Nearest Neighbors Classifier\n",
        "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "model_knn.fit(X_train_counts, y_train)\n",
        "\n",
        "# Evaluate the accuracy of each model on the testing dataset\n",
        "y_pred_lr = model_lr.predict(X_test_counts)\n",
        "y_pred_nb = model_nb.predict(X_test_counts)\n",
        "y_pred_svm = model_svm.predict(X_test_counts)\n",
        "y_pred_rf = model_rf.predict(X_test_counts)\n",
        "y_pred_bnb = model_bnb.predict(X_test_counts)\n",
        "y_pred_dt = model_dt.predict(X_test_counts)\n",
        "y_pred_knn = model_knn.predict(X_test_counts)\n",
        "\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "accuracy_bnb = accuracy_score(y_test, y_pred_bnb)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "\n",
        "# Add a title and description to the app\n",
        "st.title(\"Text Classification App\")\n",
        "st.write(\"This app can classify text into objective and subjective categories.\")\n",
        "\n",
        "# Add the accuracy of each model to the app\n",
        "st.write(f\"Logistic Regression Classifier Accuracy: {accuracy_lr}\")\n",
        "st.write(f\"Naive Bayes Classifier Accuracy: {accuracy_nb}\")\n",
        "st.write(f\"Support Vector Machine Classifier Accuracy: {accuracy_svm}\")\n",
        "st.write(f\"Random Forest Classifier Accuracy: {accuracy_rf}\")\n",
        "st.write(f\"BNB Classifier Accuracy: {accuracy_bnb}\")\n",
        "st.write(f\"Decision Tree Classifier Accuracy: {accuracy_dt}\")\n",
        "st.write(f\"K-Nearest Neighbors Classifier Accuracy: {accuracy_knn}\")\n",
        "\n",
        "\n",
        "# Add a text input field for user input\n",
        "user_input = st.text_input(\"Enter some text:\")\n",
        "\n",
        "state = SessionState()\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('results.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "# Create a table for the results if it doesn't exist\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS results\n",
        "             (text_input TEXT, predicted_label TEXT)''')\n",
        "\n",
        "# If user has entered text, predict its category using the Naive Bayes classifier and display the result\n",
        "if user_input:\n",
        "    cleaned_input = clean_text(user_input)\n",
        "    user_input_counts = vectorizer.transform([cleaned_input])\n",
        "    predicted_label_nb = model_nb.predict(user_input_counts)[0]\n",
        "        \n",
        "    state.results_df = state.results_df.append({'text_input': user_input, 'predicted_label': predicted_label_nb}, ignore_index=True)\n",
        "    \n",
        "    # Insert the results into the database\n",
        "    c.execute(\"INSERT INTO results VALUES (?, ?)\", (user_input, predicted_label_nb))\n",
        "    conn.commit()\n",
        "    \n",
        "    \n",
        "# Fetch the results from the database and display them in a table\n",
        "c.execute(\"SELECT * FROM results\")\n",
        "results = c.fetchall()\n",
        "results_df = pd.DataFrame(results, columns=['text_input', 'predicted_label'])\n",
        "st.write(results_df)\n",
        "st.write('made by Rihem Maaref')\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n",
        "\n",
        "# Add a file uploader for uploading a CSV file\n",
        "uploaded_file = st.file_uploader(\"Upload a CSV file\", type=\"csv\")\n",
        "\n",
        "\n",
        "# If a file is uploaded, label it using each classifier, save the labeled data to\n",
        "if uploaded_file is not None:\n",
        "  # Load the data from the uploaded file\n",
        "  data = pd.read_csv(uploaded_file)\n",
        "  \n",
        "  # Remove empty rows\n",
        "  data.dropna(subset=['Text'], inplace=True)\n",
        "\n",
        "  # Remove duplicates\n",
        "  data.drop_duplicates(subset=['Text'], keep='first', inplace=True)\n",
        "\n",
        "  # Preprocess the data using the CountVectorizer\n",
        "  X_counts = vectorizer.transform(data['Text'])\n",
        "\n",
        "  # Predict the category of each text using each classifier\n",
        "  predicted_labels_nb = model_nb.predict(X_counts)\n",
        "\n",
        "\n",
        "  # Add the predicted labels to the DataFrame\n",
        "  data['NB Prediction'] = predicted_labels_nb\n",
        "  labeled_data_file = \"prediction_df.csv\"\n",
        "  data.to_csv(labeled_data_file, index=False)\n",
        "  st.success(\"Data labeled successfully and saved to labeled_data.csv\")\n",
        "  st.write(\"Labeled dataset:\")\n",
        "  st.write(data)\n",
        "\n",
        "\n",
        "  # Download the labeled file\n",
        "  csv = data.to_csv(index=False)\n",
        "  b64 = base64.b64encode(csv.encode()).decode()\n",
        "  href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"labeled_data.csv\">Download labeled data</a>'\n",
        "  st.markdown(href, unsafe_allow_html=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS_3ynSDy6vF"
      },
      "outputs": [],
      "source": [
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.svm import LinearSVC\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from textblob import TextBlob\n",
        "# import base64\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.naive_bayes import BernoulliNB\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# import re\n",
        "# import string\n",
        "# import sqlite3\n",
        "\n",
        "\n",
        "# # Add a title and description to the app\n",
        "# st.title(\"Text Classification App\")\n",
        "# st.subheader(\"This app can classify text into objective and subjective categories.\")\n",
        "# st.subheader(\"Upload a CSV file or enter text to classify\")\n",
        "\n",
        "# # Use a consistent color scheme\n",
        "# st.markdown(\n",
        "#     \"\"\"\n",
        "#     <style>\n",
        "#     .stButton>button {\n",
        "#         background-color: #0072B2;\n",
        "#         color: white;\n",
        "#         border-color: #0072B2;\n",
        "#     }\n",
        "#     </style>\n",
        "#     \"\"\",\n",
        "#     unsafe_allow_html=True\n",
        "# )\n",
        "\n",
        "# # Increase the font size and use a legible font type\n",
        "# st.markdown(\n",
        "#     \"\"\"\n",
        "#     <style>\n",
        "#     body {\n",
        "#         font-size: 18px;\n",
        "#         font-family: Arial, sans-serif;\n",
        "#     }\n",
        "#     </style>\n",
        "#     \"\"\",\n",
        "#     unsafe_allow_html=True\n",
        "# )\n",
        "\n",
        "# # Add spacing between elements\n",
        "# st.write(\"\")\n",
        "# st.write(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dwsEnl5Dc_S",
        "outputId": "d409fdda-2c7e-4111-e907-8d60ddeabe39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.886s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phhA1zDQ5spj"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDcC5s-85q6N",
        "outputId": "36181ba0-5df2-48db-a097-8f1ac7c62e9a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.747s\n",
            "your url is: https://six-tigers-share-34-145-175-87.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V69GWxhgi-9W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NSxI_Y7N9zn49un3cYL23VkoF5Wb1UGT",
      "authorship_tag": "ABX9TyM2AwSFquZEVPA0p4wiIX/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}